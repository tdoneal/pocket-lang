two types of patterns:
whole-pattern: must consume the entire pattern to be valid
whole-input: must consume the entire input to be valid


token sets ideas:
set 0: characters
set 1: alphanumeric, punc, whitespace
set 2: comments, identifiers, literals (string, int, etc), keywords, syneol, syntab

stream rewriting paradigm:
stream -> stream in general:
some tokens should go to more than one output token
some output tokens should be composed of more than one input token
a common representation is hierarchical
for example we might have the syntax

main
    print "x"

and we desire the output token string

func ( stmt ( funccall ( strlit ( x ) ) ) )

if we have intermediate token set representation, keep in mind that the final token set 
must still go down to the character level, as it must somehow represent the entropy in the original source file,
most of which is likely from string or numeric literals

stream rewriting syntax
pattern-name ( > output-token-sequence ) : pattern-def
pass [args]

pass semantics: passes through all tokens specified in args

output-token-sequence syntax: 
    number (pack)
    number: output token id
    pack: optional keyword to specify to not use the defeault OUT_ID START [CHILDREN] END output pattern.
        if pack is specified, only the OUT_ID token will be emitted

the matcher is "pattern first" by default, not "token-first"
this means that the parser only "understands" things explicitly defined in the patterns,
and that no tokens are passed through by default

when a pattern matcher matches a pattern, an implicit "match tree" is created.  this tree
associates the pattern with all underlying tokens which were matched.  the tree's children
are the subpatterns of the pattern, each with their associated matched tokens.  For example
consider the pattern StringLiteral: [" *alphanumeric "] with the input tokens "hithere".
Some nodes in this match tree are named explicitly, others are implicit based on the specified pattern operators, 
such as Sequence, ManyGreedy, Disjunction.

Each pattern operator has its own match tree representation: TODO: define sensible match trees for each
primitive pattern operator (Sequence, ManyGreedy, Disjunction)

The generated match tree is:
StringLiteral(Sequence(TokenMatch("), ManyGreedy(AlphaNumeric(Disjunction()))))

General phases (pocket->go):
start format: linear list of characters (max-depth: 0)
tokenization (custom code, not that long) (output format: list of Tokens, max-depth: 1)
syntax tree-building (could be custom, but so many different node types that can be useful to use a special pattern language)
    (output format: custom tree )
tree->graph ( output format: same as tree but with some circular references)
graph type propagation (output format: same as tree)
graph execution optimization (output format: same as tree)
graph->tree (necessary?  not sure)
tokenization (output format: list of tokens)
output formatting (output format: list of characters)

simplified language: only supports scripting (whole file is the "main" function)
    only integer literals supported
    no type declarations
    only the print function supported
    only the add (+) operation supported

tree (graph) rewriting rule:
 /statement/varinit/* -> /statement/varinit/allstatements
 streaminlineop ->

 graph xform required operations:
 searching graph for a node matching a condition
    traversing all outbound edges of a node without reference to node-specific information
 replacing a node in a graph (while preserving all incoming references)
 traversing outbound edges of a node by "relationship type" (name)
 
 proposed solution: explicit edge datastructure:

 type Node struct {
    outbound[]Edge
    inbound []Edge
 }

 type Edge struct {
     name string
     Node in
     Node out
 }

 need way to represent the allowable "field names" (edge names) for each node type (done)

 a var's inferred type is equal to: the tightest type compatible with all assignments
 type inference phase: tightening only or mixture of tighten/loosen?
    - pretty sure iterative tightening will work

for example:

i : 0 (any)
i : i + 1 (any)

i : 0 [int, any]
i : i + 1 (any)

i : 0 [int, any]
i : i[int,any] + 1[int] -> i : (i + 1)[int, any]

satisfiability problem:
trying to find a type for each expression that satisfies the rules:
ltype : rtype -> (rtype < ltype)
typesum = (typea + typeb) -> (typesum==int & typea==int & typeb==int)

approach:
phase 1: compute the type constraints on each expression (the narrowest and widest each node can be)
phase 2: assign the tightest possible type to each expression

how to represent the type constraints:
idea: set of (min, max) tuples on the abstract type hierarchy.

going back to the earlier example:
phase 1:
i[0,any] : 0[0,any]
i : (i[0,any] + 1[0,any])[0,any]

i[0,any] : 0[int] # integer literals are exactly [int]
i : (i[0,any] + 1[int])[0,any] # integer literals are exactly [int]

i[int,any] : 0[int] # left-hand side must be > than right-hand side
i : (i[0,int] + 1[int])[0,int] # ops and result of add must be <int

i[int,any] : 0[int]
i : (i[0,int] + 1[int])[int] # result of add must be >max(args)

i[int] : 0[int] # intersection of independent constraints
i : (i[int] + 1[int])[int] # intersection of independent constraints

### no more rules to apply
### phase 2: pick tightest type available from each constraints
# in this case all nodes ended up on a single type already, so we're done

i[int] : 0[int] # intersection of independent constraints
i : (i[int] + 1[int])[int] # intersection of independent constraints

concept of "mypes" (metatypes)
a "mype" refers to a set of types.  I.e., there is a quickly computeable
function mype.contains(t Type) that computes whether t "satisfies" the mype

example mypes:
{/0} # the empty set of types
any # the set of all types

mypes can be compound
betweenFloatNumber = myperange[float, number]

mype propagation rules:
variable assignment: no information can be gleaned
add op: all arguments and result must be exactly int

new phases:
phase 1: compute all rvalue mype rules until quiescence
phase 2: compute variable mypes as union of all potential rvalues

i : 0
i : i + 1

i[any] : 0[int]
i[any] : [i[any] + 1[int]][any]

generate list of explicit "directions to search" (i.e., steps)
steps can have two types of static guarantees:
G1) increased specifity. the step will always make a mype more specific
G2) validity.  the step will always result in a valid type graph after running.
a step can either be 1) guaranteed to progress towards a solution (i.e., 0[any] -> 0[int])
or 2) speculative (i.e., i[any] : 0[int] -> i[int])
algorithm:
phase 1: compute list of steps with G1 and G2 (low-hanging fruit).  execute all of them 
phase 2: 
    compute list of speculative steps. 
    for each step, execute and evaluate G1 and G2
    if both G1 and G2 are satisfied, keep the result, otherwise backtrack
        In certain cases it is possible to compute G1 and/or G2 without actually performing
        any graph replacements.
repeat phases 1 and 2 until no more steps are generated

guaranteed steps examples:
literal integer -> [int]

speculative steps examples:
varRValueUnion:
    take all rvalues to a given named variable
    variable.mype = variable.mype & Union(rvalue.mype for rvalue in rvalues)
        G1: needs computing (may be idempotent assignment)
        G2: always
arithOpClosure:
    take all arguments to an add operation
    if all argument.mype are identical, then op.mype = argument[any].mype
        G1: needs computing (may be idempotent assignment)
        G2: always
varRValueAggressive:
    take a rvalue to a named variable
    variable.mype = rvalue.mype
        G1: needs computing (may be idempotent assignment)
        G2: needs computing 

i[any] : 0[any]
i : (i + 1[any])[any]
after phase 1: (guaranteed steps):
i[any] : 0[int]
i : (i + 1[int])[any]

after varRValueAggressive
i[int] : 0[int]
i : (i + 1[int])

i : 0 -> [unknown] : [int]
i : i + 1 -> [unknown] : [unknown] + [int]

[float, int, string] : [int]
[float, int, string] : [float, int, string] + [int]

# now need heuristic to choose among these options:
i[float], i[int], i[string]

# we want the one that would generate the least # of conversion code 
# and in general the least amount of B.S.
# as a human the best answer is clearly i[int], but how to decide this algorithmically?

answer 1: "killer heuristics":  e.g., find the first assignment's rvalue and use that, done

example type inference test cases:

prog:
i : 0
i : i + 1
want:
i[int]

prog:
i : 0
i : 3.4
want:
i[number]

i[any]: 0[int]
i : 3.4[float]
--> rvalue lca
i[LCA[int, float]]: 0
i : 3.4
--> lca evaluation
i[number]: 0


i[any]: 0[int]
i: (i + 1[int])[any]
--> add op compatibility
i[Disj[float, int, string]] : 0
--> add op result
i : 0
i: (i + 1)[Disj[float, int, string]]
--> var disj first assignment heuristic
i[int] : 0
i: (i + 1)
--> add op compatibility + result
i : 0
i: (i + 1)[int]



i : 0
i : i + 1
key: unknown is not the same as any
any is the set of all types, whereas unknown represents the power set of types (the set of all subsets of types)
mypes are elements of the power set of types
types are elements of the set of types
values are elements of a type


the previous type inference algorithm is valid, as long as the type checking rules assert
that unspecified parameter types work for any possible argument type

the philosophy is this:
the tie assumes each variable will have a runtime (static) type.  a mype is the set of all possible 
runtime types that a variable could be.
the tie tries to find assignments to these types that are guaranteed not to cause type runtime errors if
all the variables were instead duck typed

we need to ensure that the output program will work for all (oracle-wise) valid combinations of runtime variable values
so solution: after type narrowing, if any node is still left as mype(all), convert those to ducked types at runtime